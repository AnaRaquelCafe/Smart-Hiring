{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBA FIAP Inteligência Artificial & Machine Learning\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imagens/smart-hiring.jpg\">\n",
    "</p>\n",
    "\n",
    "## Tecnologia de Processamento de Imagens\n",
    "## Projeto Final Smart-Hiring: Entrevista Virtual\n",
    "\n",
    "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas. Por meio uma trilha guiada para construir uma aplicação que tem por objetivo analisar imagens e extrair uma série de informações que serão utilizadas para compor uma análise de seleção de candidatos para uma entrevista simulada.\n",
    "\n",
    "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
    "\n",
    "| Nome dos Integrantes     | RM            | Turma |\n",
    "| :----------------------- | :------------- | :-----: |\n",
    "| Integrante 1             | RM 12345      | `1IA` |\n",
    "| Integrante 2             | RM 12345      | `1IA` |\n",
    "| Integrante 3             | RM 12345      | `1IA` |\n",
    "| Integrante 4             | RM 12345      | `1IA` |\n",
    "\n",
    "Por ser um projeto guiado, fique atento quando houver as marcações **Implementação** indica que é necessário realizar alguma implementação em Python no bloco a seguir onde há a inscrição ```## IMPLEMENTAR``` e **Resposta** indica que é esperado uma resposta objetiva relacionado a algum questionamento. \n",
    "\n",
    "**Cada grupo pode utilizar nas respostas objetivas quaisquer itens necessários que enriqueçam seu ponto vista, como gráficos, fotos e, até mesmo, trechos de código-fonte.**\n",
    "\n",
    "Pode-se utilizar quantos blocos forem necessários para realizar determinadas implementações ou utilizá-las para justificar as respostas. Não é obrigatório utilizar somente o bloco indicado.\n",
    "\n",
    "Ao final não se esqueça de subir os arquivos do projeto nas contas do GitHub de cada membro, ou subir na do representante do grupo e os membros realizarem o fork do projeto.\n",
    "\n",
    "A avaliação terá mais ênfase nos seguintes tópicos de desenvolvimento do projeto:\n",
    " \n",
    "1. __Detector de objeto (cartão de identificação)__\n",
    "2. __Detector de faces__\n",
    "3. __Detector de sorriso__\n",
    "4. __Detector de bocejo__\n",
    "5. __Detector de olhos fechados__\n",
    "6. __Descritor de objetos na cena__\n",
    "7. __Conclusões Finais__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Disclaimer: as informações do caso de uso de negócio são meramente ilustrativas para aplicar as tecnologias de visão computacional de forma mais aderente ao desafio proposto. Todos os comentários foram forjados para dar vazão aos desafios e não representam formas de avaliação de candidatos. A empresa em questão, a Wandee, é fictícia._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A empresa **Wandee**, especializada em entrevistas virtuais, está construindo um produto minimamente viável (MVP) para testar algumas tecnologias voltadas a visão computacional para tornar o processo de seleção, especialmente a etapa de entrevista mais completo, rápido e que permita aos recrutadores obterem feedbacks mais completos além da profunidade técnica de cada posição, como por exemplo, se o candidato é ele mesmo (prova de identidade), se possuí o cartão de acesso a entrevista, aspectos de atenção durante a entrevista, como concentração e foco. Ainda será analisado questões de organização no local do entrevistado, buscando por objetos na visão da câmera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo o processo de entrevista virtual é feito remotamente por meio de uma câmera (_webcam_). Logo, todos os algoritmos desenvolvidos precisam capturar as imagens desta origem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Detector de objeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de começar com o processo de autenticação, os candidatos precisam utilizar o celular e exibir o ícone da empresa para a câmera. Se o resultado for positivo indica que o sistema pode avançar para a próxima etapa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"imagens/logo.png\" height=\"60%\" width=\"60%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construa um algortimo que seja capaz de analisar uma região de interesse específica (ROI, _Region of Interest_), capturada por vídeo, e valide se o ícone está presente ao ser exibido pelo celular do candidato. \n",
    "\n",
    "Lembre-se que questões de proporção e rotação precisam ser consideradas na identificação.\n",
    "\n",
    "Este processo precisa ser de rápida identificação, neste caso não será possível aplicar técnicas que envolvam aprendizado de máquina. É indicado o uso de detectores de objetos e extratores de características.\n",
    "\n",
    "_No seu smartphone abra o navegador e entre com o link https://raw.githubusercontent.com/michelpf/fiap-ml-tec-proc-imagens-capstone/master/projeto-final/imagens/logo.png_\n",
    "\n",
    "Ou, escaneie com seu smartphone o QRCode abaixo.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"imagens/qrcode.png\" height=\"25%\" width=\"25%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from scipy.spatial import distance as dist\n",
    "import collections\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função abaixo deve receber uma imagem capturada da região de interesse e comparar com a imagem template do logotipo de empresa. O retorno é o número de correspondências encontradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detector(imagem, template):\n",
    "    # Conversão da imagem par escala de cinza\n",
    "    imagem_norm = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # IMPLEMENTAR\n",
    "    #\n",
    "    # Escolha um detector de imagens adequado, configure e aplique um algoritmo de match\n",
    "    # Esta função deve retornar o número de correspondências de uma imagem versus seu template\n",
    "    \n",
    "\n",
    "    return len(matches)\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregue a imagem de template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Carregue a imagem do logotipo\n",
    "\n",
    "image_template = cv2.imread(None, 0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize a função ```detector``` para obter as correspondências identificadas. Por meio de testes prévios, estabeleça qual o valor de _matches_ para o template definido. Isto pode fazer com que ajuste valores do detector ORB para ajustes, é um processo de experimentação.\n",
    "\n",
    "Após definir o limiar, desenvolva uma regra para comparar com o valor de _matches_ e exibir em tempo real se o template foi localizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função de suporte para exibição de imagens no Jupyter\n",
    "\n",
    "def exibir_imagem(imagem):\n",
    "    figure(num=None, figsize=(15, 10))\n",
    "    image_plt = mpimg.imread(imagem)\n",
    "    plt.imshow(image_plt)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O trecho de códido abaixo é para iniciar a captura de imagens da câmera. Nela será definido uma região de interesse que deverá ser capturado uma imagem para acionar a função de detecção."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Obtendo imagem da câmera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Definindo região de interesse (ROI)\n",
    "        height, width = frame.shape[:2]\n",
    "        top_left_x = int(width / 3)\n",
    "        top_left_y = int((height / 2) + (height / 4))\n",
    "        bottom_right_x = int((width / 3) * 2)\n",
    "        bottom_right_y = int((height / 2) - (height / 4))\n",
    "    \n",
    "        # Desenhar retângulo na região de interesse\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), 255, 3)\n",
    "\n",
    "        # Obtendo região de interesse para validação do detector\n",
    "        cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "\n",
    "        # Executando o detector, definindo um limiar e fazendo a comparação para validar se o logotipo foi detectado\n",
    "        # IMPLEMENTAR\n",
    "        \n",
    "       \n",
    "\n",
    "        cv2.imshow(\"Identificacao de Template\", frame)\n",
    "        \n",
    "    # Se for teclado Enter (tecla 13) deverá sair do loop e encerrar a captura de imagem\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazene uma evidência do logotipo detectado, exibindo na imagem a região de interesse com a imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Passe o parâmetro localização da imagem para exibi-la no notebook\n",
    "\n",
    "exibir_imagem(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detector de faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para validação de autenticidade do candidado, o processo de entrevista virtual precisa confirmar se a pessoa selecionada para a entrevista é a mesma. Neste caso a técnica a ser utilizada é por meio de um reconhecimento facial,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inicialmente, precisamos treinar um classificador próprio do OpenCV com exemplos de imagem do candidato.\n",
    "Eleja uma pessoa do grupo para ser o candidato e treine um conjunto de imagens suficiente para que seja possível alcançar similaridade, onde o valor de não similaridade seja de até 40 pontos. O algortimo de similaridade de faces utiliza um sistema que quando a face é idêntica, o número de pontos é igual a 0, se for totalmente diferente, tende ao infinito. Logo, patamar de 30 a 40 é um bom número de similaridade."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementação**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nesta etapa inicial será realizado o treinamento das faces de um determinado candidato.\n",
    "Você precisará coletar um número de imagens relevante do candidato. Além disso, é recomendável aplicação de um detector de faces para que seja extraído somente a _região de interesse_ ou seja, a própria face. Uma maneira de conseguir este tipo de segmentação é utilizando um classificador em cascada de Haar treinado para este fim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize a função a abaixo para segmentar o rosto de uma imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_classifier = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def face_extractor(img):\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_classifier.detectMultiScale(gray, 1.2, 5)\n",
    "    \n",
    "    if faces is ():\n",
    "        return None\n",
    "    \n",
    "    for (x,y,w,h) in faces:\n",
    "        cropped_face = img[y:y+h, x:x+w]\n",
    "\n",
    "    return cropped_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie um algoritmo para que treine um determinado número de faces, escolhido pelo grupo, par que seja armazenado em um diretório específico para posteior treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "contagem = 0\n",
    "\n",
    "# IMPLEMENTAR\n",
    "# Defina o número máximo de imagens a serem coletadas\n",
    "\n",
    "contagem_maxima = 100\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        cv2.imshow(\"Imagem de Treino\", frame)\n",
    "        \n",
    "        # IMPLEMENTAR\n",
    "        # Crie um algoritmo para salvar as imagens segmentadas em face em um determinado diretório\n",
    "        \n",
    "        # Se for teclado Enter (tecla 13) deverá sair do loop e encerrar a captura de imagem\n",
    "        # ou for alcançado a contagem máxima (amostras)\n",
    "        if cv2.waitKey(1) == 13 or contagem == contagem_maxima:\n",
    "            break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "print(\"Coleta de amostras completado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O treinamento utilizando é um próprio classificador de faces que o OpenCV possui. Neste caso vamos optar pelo classificador Local Binary Patterns Histograms (LBPH), que para este cenário é o mais adequado.\n",
    "\n",
    "O grupo pode optar por escolher outros tipos de algoritmos do OpenCV, se desejarem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Defina o diretório utilizado para salvar as faces de exemplo\n",
    "\n",
    "data_path = None\n",
    "\n",
    "onlyfiles = [f for f in listdir(data_path) if isfile(join(data_path, f))]\n",
    "training_data, labels = [], []\n",
    "\n",
    "for i, files in enumerate(onlyfiles):\n",
    "    image_path = data_path + onlyfiles[i]\n",
    "    images = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    training_data.append(images)\n",
    "    labels.append(0)\n",
    "\n",
    "# Criando uma matriz da lista de labels\n",
    "labels = np.asarray(labels, dtype=np.int32)\n",
    "\n",
    "# Treinamento do modelo\n",
    "model = cv2.face.LBPHFaceRecognizer_create()\n",
    "model.train(training_data, labels)\n",
    "\n",
    "print(\"Modelo treinado com sucesso.\")\n",
    "\n",
    "# IMPLEMENTAR\n",
    "# Defina na chave 0 o nome do candidato\n",
    "\n",
    "persons = {0: None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos considerar uma classificação com sucesso quando a distândia de predição da face analisada for entre 30 e 40. Os valores de retorno destes classificador não é um índice de confiança.\n",
    "\n",
    "Quando houver uma deteção dentro da margem de distância, armazene a imagem com o nome \"success_candidate.png\", constando as informações do nome do candidato e a distância identificada pelo classificador (retorno do método _predict_)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escreva na tela onde a imagem da câmera é capturada a informação da distância de detecção da face extraída versus a face treinada, emuldure a face num retângulo e, se a face for devidadamente identificada, inclua um texto com a informação \"```<Nome do candidato>``` Reconhecido\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        \n",
    "        # IMPLEMENTAR\n",
    "        # Extraia a face da imagem obtida da câmera\n",
    "        # Faça os ajustes necessários para classificá-la no classifcador treinado\n",
    "        # Estabeleça um algoritmo para concluir se o resultado é 'Sucesso', candidato identificado ou 'Não Indetificado' para\n",
    "        # quando não for localizado o candidato\n",
    "        # Analise também situações onde a face não é identificada\n",
    "        # Utilize a função face_extractor para segmentar a imagem de rosto\n",
    "        \n",
    "        \n",
    "    # Se for teclado Enter (tecla 13) deverá sair do loop e encerrar a captura de imagem    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Guarde como evidência de classificação bem sucedida, uma imagem capturada da câmera durante o processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Passe o parâmetro localização da imagem para exibi-la no notebook\n",
    "\n",
    "exibir_imagem(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Detector de sorriso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas empresas demandam que futuros empregados demonstrem educação, tranquilidade e empatia, características que podem ser evidencias com uma uma análise simples da receptividade do candidado por meio das expressões de seu rosto.\n",
    "\n",
    "Um detector importante disto é sobre se o candidato mantém o semblante fechado ou se mantém um rosto alegre, oscilando sorrisos a medida que é realizada a entrevista e demais questionamentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize as bibliotecas do _DLib_, em especial o preditor treinado para 68 pontos de marcação de face, para identificar a geometria dela e obtenha as marcações de interesse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uma das formas de extraírmos os pontos de contorno da face é utilizando o modelo do _DLib_ ```shape_predictor_68_face_landmarks.dat```. Este modelo retorna 68 pontos da face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import numpy \n",
    "\n",
    "predictor_68_path = \"modelos/shape_predictor_68_face_landmarks.dat\"\n",
    "\n",
    "predictor = dlib.shape_predictor(predictor_68_path)\n",
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Liste os pontos de cada parte do rosto. A partir deles poderão ser feitos estudos geométricos para identificar características relcioandos aos mesmos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Para cada constante abaixo, indique uma lista de pontos dos 68 identificados pelo classificador do DLib\n",
    "FACE_POINTS = None\n",
    "MOUTH_POINTS = None\n",
    "RIGHT_BROW_POINTS = None\n",
    "LEFT_BROW_POINTS = None\n",
    "RIGHT_EYE_POINTS = None\n",
    "LEFT_EYE_POINTS = None\n",
    "NOSE_POINTS = None\n",
    "JAW_POINTS = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie uma função que será utilizada para identificar um sorriso. Leve em consideração estudos que envolvem cálculo de razão de aspecto geométrico e adapte para os pontos dos lábios.\n",
    "\n",
    "Estude o paper de [Soukupová e Čech de 2016](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf) para entender como obter um cálculo de aspecto de razão para formas geométricas da face. Você pode fazer aproximações a partir do estudo do paper para outras formas, como os lábios e olhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crie uma função ```month_aspect_ratio``` que receba os pontos dos lábios e calcule o aspecto de razão para que seja exibida dados de quando a boca está aberta, fechada, dentre outros comportamentos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def month_aspect_ratio(mouth):\n",
    "    \n",
    "    # IMPLEMENTAR\n",
    "    # Utilize referências do paper para calcular o MAR (Mouth Aspect Ratio)\n",
    "    \n",
    "    return mar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na função abaixo, inclua um segundo retorno que será a razão de aspecto dos lábios. Deixe como está o terceiro retorno, pois ele será estudado no próximo algoritmo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_landmarks_convex_hull_image(im):\n",
    "    im = im.copy()\n",
    "    rects = detector(im, 1)\n",
    "    \n",
    "    if len(rects) == 0:\n",
    "        return im, 0, 0\n",
    "    \n",
    "    landmarks_list = []\n",
    "    \n",
    "    for rect in rects:\n",
    "        landmarks = numpy.matrix([[p.x, p.y] for p in predictor(im, rect).parts()])\n",
    "\n",
    "        for k, d in enumerate(rects):\n",
    "            cv2.rectangle(im, (d.left(), d.top()), (d.right(), d.bottom()), (0, 255, 0), 2)\n",
    "\n",
    "            points = cv2.convexHull(landmarks[NOSE_POINTS])\n",
    "            cv2.drawContours(im, [points], 0, (0, 255, 0), 1)\n",
    "\n",
    "            points = cv2.convexHull(landmarks[MOUTH_POINTS])\n",
    "            cv2.drawContours(im, [points], 0, (0, 255, 0), 1)\n",
    "            \n",
    "            points = cv2.convexHull(landmarks[RIGHT_BROW_POINTS])\n",
    "            cv2.drawContours(im, [points], 0, (0, 255, 0), 1)\n",
    "\n",
    "            points = cv2.convexHull(landmarks[LEFT_BROW_POINTS])\n",
    "            cv2.drawContours(im, [points], 0, (0, 255, 0), 1)\n",
    "\n",
    "            points = cv2.convexHull(landmarks[RIGHT_EYE_POINTS])\n",
    "            cv2.drawContours(im, [points], 0, (0, 255, 0), 1)\n",
    "            \n",
    "            points = cv2.convexHull(landmarks[LEFT_EYE_POINTS])\n",
    "            cv2.drawContours(im, [points], 0, (0, 255, 0), 1)\n",
    "            \n",
    "            month_aspect = None\n",
    "            eye_aspect = None\n",
    "\n",
    "    return im, month_aspect, eye_aspect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realize ensaios para definir o valor de sorriso versos simulações com os lábios normais e aberto. Um sorriso é uma estado entre os lábios fechados ou semi-fechados e a boca inteiramente aberta.\n",
    "Definina abaixo os limiares inferior e superior para a identificação de um sorriso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Defina os valores mínimo e máximo para detecção do sorriso\n",
    "\n",
    "sorriso_minimo = None\n",
    "sorrimo_maximo = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Após identificar o sorriso, contabilize quantas vezes foram identificados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_capture.release()\n",
    "cam_capture = cv2.VideoCapture(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Quantidade de sorrisos identificados\n",
    "qtde_sorrisos = 0\n",
    "\n",
    "while True:\n",
    "    ret, image_frame = cam_capture.read()\n",
    "    prev_sorriso = sorriso \n",
    "    \n",
    "    if ret:\n",
    "        image_frame, month_aspect, _ = annotate_landmarks_convex_hull_image(image_frame)\n",
    "        \n",
    "        # IMPLEMENTAR\n",
    "        # Defina o algoritmo que irá identificar o sorriso baseado nos limites defindos\n",
    "        # Crie uma lógica para contar quantas vezes o sorriso foi dado\n",
    "     \n",
    "        cv2.imshow(\"Detector de Sorriso\", image_frame)\n",
    "        \n",
    "        # Se for teclado Enter (tecla 13) deverá sair do loop e encerrar a captura de imagem   \n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazene um exemplo de uma imagem, na pasta ```imagens``` com o sorriso detectado para exibição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Passe o parâmetro localização da imagem para exibi-la no notebook\n",
    "\n",
    "exibir_imagem(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Detector de bocejos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os candidados devem estar sempre atentos durante a entrevista virtual. Para garantir que ele se preparou adequadamente antes do início da entrevista não deverá ser tolerado bocejos.\n",
    "\n",
    "Um detector de bocejos deverá utilizar aspectos das marcações dos lábios já definidas para identificar o bocejo. Neste caso o que será diferente é o valor da razão de aspecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste caso não há um limiar, como o bocejo é a boca aberta ao máximo, vamos definir um valor mínimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Defina o valor mínimo de abertura dso lábios\n",
    "\n",
    "bocejo_minimo = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_capture.release()\n",
    "    \n",
    "cam_capture = cv2.VideoCapture(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Quantidade de bocejos identificados\n",
    "qtde_bocejos = 0\n",
    "\n",
    "while True:\n",
    "    ret, image_frame = cam_capture.read()\n",
    "    prev_bocejo = bocejo \n",
    "    \n",
    "    if ret:\n",
    "        image_frame, month_aspect, _ = annotate_landmarks_convex_hull_image(image_frame)\n",
    "        \n",
    "        # IMPLEMENTAR\n",
    "        # Defina o algoritmo que irá identificar o bocejo baseado no limite defindo\n",
    "        # Crie uma lógica para contar quantas vezes o bocejo foi dado\n",
    "        \n",
    "        cv2.imshow(\"Detector de Bocejos\", image_frame)\n",
    "       \n",
    "        # Se for teclado Enter (tecla 13) deverá sair do loop e encerrar a captura de imagem   \n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armazene um exemplo de uma imagem, na pasta ```imagens``` com o bocejo detectado para exibição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Passe o parâmetro localização da imagem para exibi-la no notebook\n",
    "\n",
    "exibir_imagem(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detector de olhos fechados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A atenção durante um processo de entrevista é algo crucial, e mais marcante neste etapa do processo seletivo. Por esta razão é preciso identificar a quantidade de vezes que o entrevistado feche os olhos, para entendermos se ele de fato está atento as perguntas e ao processo como um todo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elabore um algoritmo que detecte os olhos fechados e contabilize ao final da transmissão."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este caso requer um estudo também geométrico que visa analisar os pontos da marcação dos olhos. Para fins de simplificação, podemos adotar um único olho, e a partir dele, estebelecer o razão de aspecto para quando ele está aberto e fechado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construa uma função chamada ```eye_aspect_ratio``` para calcular o aspecto de razão de um dos olhos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eye_aspect_ratio(eye):\n",
    "    \n",
    "    # IMPLEMENTAR\n",
    "    # Calcule o EAR (Eye Aspect Ratio) que determina o aspecto de razão da geometria de um dos olhos\n",
    "    \n",
    "    return ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altere a função ```annotate_landmarks_convex_hull_image``` para exibir, no terceiro parâmetro o valor de aspecto de um dos olhos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No caso do olho, precisamos definir somente um valor máximo de limite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Defina um valor máximo para determinar se o olho está fechado\n",
    "olho_maximo = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_capture.release()\n",
    "    \n",
    "cam_capture = cv2.VideoCapture(0)\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "#Quantidade de olhos fechados identificados\n",
    "qtde_olhos_fechados = 0\n",
    "\n",
    "while True:\n",
    "    ret, image_frame = cam_capture.read()\n",
    "    prev_olhos_fechados = olhos_fechados \n",
    "    \n",
    "    if ret:\n",
    "        image_frame, _, eye_aspect = annotate_landmarks_convex_hull_image_smile(image_frame)\n",
    "        \n",
    "        # IMPLEMENTAR\n",
    "        # Defina o algoritmo que irá identificar se um olho está fechado baseado no limite defindo\n",
    "        # Crie uma lógica para contar quantas vezes o olho foi fechado\n",
    "        \n",
    "        cv2.imshow(\"Detector de Olhos Fechados\", image_frame)\n",
    "        \n",
    "        # Se for teclado Enter (tecla 13) deverá sair do loop e encerrar a captura de imagem  \n",
    "        if cv2.waitKey(1) == 13:\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Descritor de objetos na cena"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A organização do local na casa do entrevistado é um item relevante, pois a partir destes detalhes é possível traçar alguns tipos de perfis que são essenciais para certas posições nas empresas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta forma precisamos construir um algoritmo que realize uma inspeção de objetos na área da câmera que é utilizada para fazer a entrevista. Ao final mostre quais e quantos objetos foram detectados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "É necessário baixar os pesos (modelo de deep-learning) neste link https://pjreddie.com/media/files/yolov3.weights e copiar para  pasta weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from darknet import Darknet\n",
    "\n",
    "# Configurações na rede neural YOLOv3\n",
    "cfg_file = 'cfg/yolov3.cfg'\n",
    "m = Darknet(cfg_file)\n",
    "\n",
    "# Pesos pré-treinados\n",
    "weight_file = 'C:/Users/Michel.Fernandes/Downloads/yolov3.weights'\n",
    "m.load_weights(weight_file)\n",
    "\n",
    "# Rótulos de classes\n",
    "namesfile = 'data/coco.names'\n",
    "class_names = load_class_names(namesfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ajuste os valores de NMS (_Non-Maximum Supression_) para regular a sensibilidade de imagens com baixa luminosidade e IOU (_Intersect of Union_) que definie o indicador se o retângulo de identificação de imagem foi adequadamente desenhado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Definia apropriadamente os valores de limiar de NMS e IOU\n",
    "\n",
    "nms_thresh = None\n",
    "iou_thresh = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separe um imagem que será analisada pelo classificador, após teclar o _Enter_. Armazene no diretório ```imagens/local-entrevista.png```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, image_frame = cam_capture.read()\n",
    "    \n",
    "    if ret:\n",
    "        cv2.imshow(\"Inspecao Local\", image_frame)\n",
    "        \n",
    "        # IMPLEMENTAR\n",
    "        # Após teclar enter, armazene uma imagem para posterior análise\n",
    "        \n",
    "cam_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A imagem a ser analisada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Altere abaixo o parâmetro de entrada para o caminho onde a imagem foi armazenada\n",
    "\n",
    "exibir_imagem(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os passos abaixo são para configuração da imagem no padrão que o classificador foi treinado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo tamnaho do gráfico\n",
    "plt.rcParams['figure.figsize'] = [24.0, 14.0]\n",
    "\n",
    "# Carregar imagem para classificação\n",
    "img = cv2.imread('imagens/local-entrevista.png')\n",
    "\n",
    "# Conversão para o espaço RGB\n",
    "original_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Redimensionamento para adatapção da primeira camada da rede neural \n",
    "resized_image = cv2.resize(original_image, (m.width, m.height))\n",
    "\n",
    "# Deteteção de objetos na imagem\n",
    "boxes = detect_objects(m, resized_image, iou_thresh, nms_thresh)\n",
    "\n",
    "# Desenho no gráfico com os regângulos e rótulos\n",
    "plot_boxes(original_image, boxes, class_names, plot_labels = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtenha os objetos identificados a partir da função ```list_objects(boxes, class_names)```. Será retornardo uma lista de objetos que deverá ser analisado para contar a quantidade de cada objeto. Se houver mais de um item igual, por exemplo _tvmonitor_ ele aparecerá duplicado na imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "# Conte os objetos identificados pelo classificador, de forma que seja exibido \n",
    "# objeto 1, quantidade 1\n",
    "# objeto 2, quantidade 1\n",
    "# ...\n",
    "\n",
    "objetos = list_objects(boxes, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusões finais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta:** Diante de todos os desafios propostos (1 ao 6) e soluções encontradas, quais seriam os próximos passos de forma a tornar mais precisos cada atividade, levando em consideração: (1) restrições de processamento em tempo real, (2) sem restrições de processamento em tempo real?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
