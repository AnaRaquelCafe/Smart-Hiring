{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MBA FIAP Inteligência Artificial & Machine Learning\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/smart-hiring.jpg\">\n",
    "</p>\n",
    "\n",
    "## Tecnologia de Processamento de Imagens\n",
    "## Projeto Final Smart-Hiring: Entrevista Virtual\n",
    "\n",
    "Este projeto final tem como objetivo explorar os conhecimentos adquiridos nas aulas práticas. Por meio uma trilha guiada para construir uma aplicação que tem por objetivo analisar imagens e extrair uma série de informações que serão utilizadas para compor uma análise de seleção de candidatos para uma entrevista simulada.\n",
    "\n",
    "Este projeto poderá ser feita por grupos de até 4 pessoas.\n",
    "\n",
    "| Nome dos Integrantes     | RM            | Turma |\n",
    "| :----------------------- | :------------- | :-----: |\n",
    "| Integrante 1             | RM 12345      | `1IA` |\n",
    "| Integrante 2             | RM 12345      | `1IA` |\n",
    "| Integrante 3             | RM 12345      | `1IA` |\n",
    "| Integrante 4             | RM 12345      | `1IA` |\n",
    "\n",
    "Por ser um projeto guiado, fique atento quando houver as marcações **Implementação** indica que é necessário realizar alguma implementação em Python no bloco a seguir onde há a inscrição ```##IMPLEMENTAR``` e **Resposta** indica que é esperado uma resposta objetiva relacionado a algum questionamento. \n",
    "\n",
    "**Cada grupo pode utilizar nas respostas objetivas quaisquer itens necessários que enriqueçam seu ponto vista, como gráficos, fotos e, até mesmo, trechos de código-fonte.**\n",
    "\n",
    "Pode-se utilizar quantos blocos forem necessários para realizar determinadas implementações ou utilizá-las para justificar as respostas. Não é obrigatório utilizar somente o bloco indicado.\n",
    "\n",
    "Ao final não se esqueça de subir os arquivos do projeto nas contas do GitHub de cada membro, ou subir na do representante do grupo e os membros realizarem o fork do projeto.\n",
    "\n",
    "A avaliação terá mais ênfase nos seguintes tópicos de desenvolvimento do projeto:\n",
    " \n",
    "1. __Detector de objeto (cartão de identificação)__\n",
    "2. __Detector de faces__\n",
    "3. __Detector de sorriso__\n",
    "4. __Detector de bocejo__\n",
    "5. __Detector de olhos fechados__\n",
    "6. __Descritor de objetos na cena__\n",
    "7. __Conclusões Finais__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introdução"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Disclaimer: as informações do caso de uso de negócio são meramente ilustrativas para aplicar as tecnologias de visão computacional de forma mais aderente ao desafio proposto._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A empresa **Wandee**, especializada em entrevistas virtuais, está construindo um produto minimamente viável (MVP) para testar algumas tecnologias voltadas a visão computacional para tornar o processo de entrevista mais completo, rápido e que permita aos recrutadores obterem feedbacks além da profunidade técnica de cada posição, como por exemplo, se o candidato é ele mesmo (prova de identidade), se possuí o cartão de acesso a entrevista, aspectos de atenção durante a entrevista, como concentração e atenção. Ainda será analisado questões de organização no local do entrevistado, buscando por objetos na visão da câmera."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detector de objeto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de começar com o processo de autenticação, os candidatos precisam utilizar o celular e exibir o ícone de SmartHire para a câmera. Se o resultado for positivo indica que o sistema pode avançar para a próxima etapa que é o reconhecimento facial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"img/logo.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construa um algortimo que seja capaz de analisar uma região de interesse específica, capturada por vídeo, e valide se o ícone está presente ao ser exibido pelo celular do candidato. \n",
    "\n",
    "Lembre-se que questões de proporção e rotação precisam ser consideradas na identificação.\n",
    "\n",
    "Este processo precisa ser de rápida identificação, neste caso não será possível aplicar técnicas que envolvam aprendizado de máquina. É indicado o uso de detectores de objetos e extratores de características.\n",
    "\n",
    "_No seu smartphone abra o navegador e entre com o link https://raw.githubusercontent.com/michelpf/fiap-ml-tec-proc-imagens-capstone/master/projeto-final/img/logo.png_\n",
    "\n",
    "Ou, escaneie com seu smartphone o QRCode abaixo.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"img/qrcode.png\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementação** \n",
    "\n",
    "Carregue o dataset \"automobile-mod.csv\" que se encontra na pasta \"data\" e faça uma inspeção nas 10 primeiras linhas para identificação básica dos atributos.\n",
    "\n",
    "**O dataset original \"automobile.csv\" se encontra na mesma pasta apenas como referência. Não deverá ser utilizado.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure adequadamente o detector ORB para que seja capaz de lidar com rotações e redimensionamentos da imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detector_ORB(imagem, template):\n",
    "    # Conversão da imagem par escala de cinza\n",
    "    imagem_norm = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Criacão do detector de ORB\n",
    "    # IMPLEMENTAR\n",
    "\n",
    "    # Identificação de pontos chave da imagem a ser validada\n",
    "    (kp1, des1) = orb.detectAndCompute(imagem_norm, None)\n",
    "\n",
    "    # Identificação de pontos chave da imagem template\n",
    "    (kp2, des2) = orb.detectAndCompute(template, None)\n",
    "\n",
    "    # Criação de correpondência entre as imagens\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # Execução de correspondência\n",
    "    matches = bf.match(des1,des2)\n",
    "\n",
    "    return len(matches)\n",
    "\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementar**\n",
    "\n",
    "- Carregue a imagem de template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENTAR\n",
    "image_template = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize a função ```detector_ORB``` para obter as correspondências identificadas. Por meio de testes prévios, estabeleça qual o valor de _matches_ para o template definido. Isto pode fazer com que ajuste valores do detector ORB para ajustes, é um processo de experimentação.\n",
    "\n",
    "Após definir o limiar, desenvolva uma regra para comparar com o valor de _matches_ e exibir em tempo real se o template foi localizado.\n",
    "\n",
    "**Implementar**\n",
    "\n",
    "- Executar função\n",
    "- Definir limiar\n",
    "- Exibir em tempo real o resultado da comparação da imagem obtida na webcam com o template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Obtendo imagem da câmera\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        \n",
    "        # Definindo região de interesse (ROI) proporcional ao tamanho da tela\n",
    "        height, width = frame.shape[:2]\n",
    "        top_left_x = int(width / 3)\n",
    "        top_left_y = int((height / 2) + (height / 4))\n",
    "        bottom_right_x = int((width / 3) * 2)\n",
    "        bottom_right_y = int((height / 2) - (height / 4))\n",
    "    \n",
    "        # Desenhar retângulo na região de interesse\n",
    "        cv2.rectangle(frame, (top_left_x,top_left_y), (bottom_right_x,bottom_right_y), 255, 3)\n",
    "\n",
    "        # Obtendo região de interesse para validação do detector\n",
    "        cropped = frame[bottom_right_y:top_left_y , top_left_x:bottom_right_x]\n",
    "\n",
    "        # Executando o detector\n",
    "        # IMPLEMENTAR\n",
    "    \n",
    "        cv2.imshow(\"Identificacao de Template\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #Tecla Enter para sair\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pergunta**: Como foi o processo de ajuste dos valores do detector ORB? Como foi a relação de números de pontos máximo detectado e a escala de tamanho versus os resultados? Mais pontos garantem melhor detecção? Há algum tradeoff relevante?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resposta**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
